{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e79e944-9bc8-4337-8eda-772796863c3c",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be0754-2262-49c1-b4c7-1d358456a491",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff33f49-61ce-4919-90c0-60a37e28c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import f1_score as ner_f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import get_scheduler, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d6f66-6197-47e9-b62d-2bc61b0d0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:1'\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_RATIO = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a9fb3-4973-4501-965a-efb4275eed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['O', 'B-Entity', 'I-Entity']\n",
    "NUM_LABELS = len(LABELS)\n",
    "LABEL2ID = {l:i for i,l in enumerate(LABELS)}\n",
    "ID2LABEL = {i:l for i,l in enumerate(LABELS)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0f843-3397-4626-b12e-30e0add01a1c",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9bf71-daf5-42ef-94d7-75fae947d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mention_span(text, mention):\n",
    "    spans = []\n",
    "    \n",
    "    gs = 0\n",
    "    while True:\n",
    "        s = text.find(mention)\n",
    "        if s == -1:\n",
    "            break\n",
    "        e = s + len(mention)\n",
    "        \n",
    "        spans.append((gs+s, gs+e))\n",
    "        gs = gs+e\n",
    "        text = text[e:]\n",
    "\n",
    "    return spans\n",
    "\n",
    "\n",
    "def pad_sequences(seqs, pad_val, maxlen):     \n",
    "    _maxlen = max([len(s) for s in seqs])\n",
    "    maxlen = min(maxlen, _maxlen) if maxlen else _maxlen \n",
    "    \n",
    "    padded_seqs = []\n",
    "    for seq in seqs:\n",
    "        pads = [pad_val] * (maxlen - len(seq))\n",
    "        seq = seq + pads\n",
    "        padded_seqs.append(seq)\n",
    "\n",
    "    padded_seqs = torch.tensor(padded_seqs)\n",
    "    return padded_seqs\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        entity = np.random.choice(item['entities'])\n",
    "        entity_type = entity['entity_type']\n",
    "        entity_mentions = entity['entity_mentions']\n",
    "\n",
    "        inputs = self.tokenizer(text, entity_type, truncation='only_first', max_length=self.max_length)\n",
    "        label = [0 for _ in range(len(inputs.input_ids))]\n",
    "        for m in entity_mentions:\n",
    "            spans = find_mention_span(text, m)\n",
    "            for s, e in spans:\n",
    "                s = inputs.char_to_token(s)\n",
    "                e = inputs.char_to_token(e - 1)\n",
    "                if s is None or e is None: \n",
    "                    continue\n",
    "                \n",
    "                label[s] = 1 # B-Entity\n",
    "                for i in range(s+1, e+1):\n",
    "                    label[i] = 2 # I-Entity\n",
    "        \n",
    "        return inputs['input_ids'], inputs['attention_mask'], label\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_ids, attention_mask, labels = zip(*batch)\n",
    "        input_ids = pad_sequences(input_ids, self.tokenizer.pad_token_id, self.max_length)\n",
    "        attention_mask = pad_sequences(attention_mask, 0, self.max_length)\n",
    "        labels = pad_sequences(labels, -100, self.max_length)\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "    def get_dataloader(self, batch_size, shuffle):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size, shuffle=shuffle, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b134f-4a30-478a-8bb1-3f0ecd29770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488ddcd-0cf4-4823-aa87-4241d42be205",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('yongsun-yoon/open-ner-english')\n",
    "train_data = data['train']\n",
    "valid_data = data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a96ce-da13-44a8-8d2e-a35ab657a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data, tokenizer, MAX_LENGTH)\n",
    "train_loader = train_dataset.get_dataloader(BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = Dataset(valid_data, tokenizer, MAX_LENGTH)\n",
    "valid_loader = valid_dataset.get_dataloader(BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e6db4-21e8-4352-8ea9-fa0a1e82eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, labels = next(iter(train_loader))\n",
    "input_ids.shape, attention_mask.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d698905-7eeb-4688-b9d3-e77f7640450f",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8f750-984a-4a0b-afb8-0861779e4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_preds, total_labels = [], []\n",
    "    for input_ids, attention_mask, labels in tqdm(loader):\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu()\n",
    "\n",
    "        bs = preds.shape[0]\n",
    "        for i in range(bs):\n",
    "            pred, label = preds[i], labels[i]\n",
    "            idxs = torch.where(label != -100)\n",
    "            total_preds.append(pred[idxs].tolist())\n",
    "            total_labels.append(label[idxs].tolist())\n",
    "            \n",
    "    return total_preds, total_labels\n",
    "\n",
    "\n",
    "def token_f1_func(total_preds, total_labels):\n",
    "    y_pred = list(itertools.chain(*total_preds))\n",
    "    y_true = list(itertools.chain(*total_labels))\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "\n",
    "def entity_f1_func(total_preds, total_labels, LABELS):\n",
    "    y_pred = [[LABELS[p] for p in preds] for preds in total_preds]\n",
    "    y_true = [[LABELS[l] for l in labels] for labels in total_labels]\n",
    "    return ner_f1_score(y_true, y_pred, average=\"macro\", mode=\"strict\", scheme=IOB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18809a06-7581-4ee3-99ff-103d0f8787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, label2id=LABEL2ID, id2label=ID2LABEL)\n",
    "_ = model.train().to(DEVICE)\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = get_scheduler('cosine', optimizer, num_training_steps, num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c17d0-710d-4053-81eb-51b4a24bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask, labels = [b.to(DEVICE) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        log = {'loss': loss.item()}\n",
    "        pbar.set_postfix(log)\n",
    "\n",
    "    total_preds, total_labels = predict(model, valid_loader)\n",
    "    entity_f1 = entity_f1_func(total_preds, total_labels, LABELS)\n",
    "    token_f1 = token_f1_func(total_preds, total_labels)\n",
    "    print(f'ep {ep:02d} | entity_f1 {entity_f1:.3f} | token_f1 {token_f1:.3f}')\n",
    "\n",
    "    if entity_f1 > best_score:\n",
    "        tokenizer.save_pretrained('ckpt')\n",
    "        model.save_pretrained('ckpt')\n",
    "        best_score = entity_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ee26c-03be-44b6-b5eb-9f872f2c29dd",
   "metadata": {},
   "source": [
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d245a-e5b1-47ec-9d0b-9511cced97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    flattened_data = []\n",
    "    for d in data:\n",
    "        for entity in d['entities']:\n",
    "            flattened_data.append({\n",
    "                'text': d['text'],\n",
    "                'entity_type': entity['entity_type'],\n",
    "                'entity_mentions': entity['entity_mentions']\n",
    "            })\n",
    "    return HFDataset.from_list(flattened_data)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        entity_type = item['entity_type']\n",
    "        entity_mentions = item['entity_mentions']\n",
    "\n",
    "        inputs = self.tokenizer(text, entity_type, truncation='only_first', max_length=self.max_length)\n",
    "        label = [0 for _ in range(len(inputs.input_ids))]\n",
    "        for m in entity_mentions:\n",
    "            spans = find_mention_span(text, m)\n",
    "            for s, e in spans:\n",
    "                s = inputs.char_to_token(s)\n",
    "                e = inputs.char_to_token(e - 1)\n",
    "                if s is None or e is None: \n",
    "                    continue\n",
    "                \n",
    "                label[s] = 1 # B-Entity\n",
    "                for i in range(s+1, e+1):\n",
    "                    label[i] = 2 # I-Entity\n",
    "        \n",
    "        return inputs['input_ids'], inputs['attention_mask'], label\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_ids, attention_mask, labels = zip(*batch)\n",
    "        input_ids = pad_sequences(input_ids, self.tokenizer.pad_token_id, self.max_length)\n",
    "        attention_mask = pad_sequences(attention_mask, 0, self.max_length)\n",
    "        labels = pad_sequences(labels, -100, self.max_length)\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "    def get_dataloader(self, batch_size, shuffle):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size, shuffle=shuffle, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d9990-6d27-4f6f-9397-fa752b4ba7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ckpt')\n",
    "model = AutoModelForTokenClassification.from_pretrained('ckpt')\n",
    "_ = model.eval().requires_grad_(False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84ad10-fa56-4c5b-8c3e-a220332d794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('yongsun-yoon/open-ner-english')\n",
    "valid_data = data['validation']\n",
    "valid_data = flatten_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89706aab-4883-4c72-87f5-6234cf2199f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = Dataset(valid_data, tokenizer, MAX_LENGTH)\n",
    "valid_loader = valid_dataset.get_dataloader(BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290791ed-8255-4233-b6a8-98dc37224179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_f1 0.560 | token_f1 0.747\n",
    "total_preds, total_labels = predict(model, valid_loader)\n",
    "entity_f1 = entity_f1_func(total_preds, total_labels, LABELS)\n",
    "token_f1 = token_f1_func(total_preds, total_labels)\n",
    "print(f'entity_f1 {entity_f1:.3f} | token_f1 {token_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d66699-7d8f-4a85-a7f3-9cb94f376ffe",
   "metadata": {},
   "source": [
    "## 5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c549bf-31d1-4358-9876-fcded22dc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(nlp, text, entity_type):\n",
    "    input_text = f'{text}{nlp.tokenizer.sep_token}{entity_type}'\n",
    "    return nlp(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7913a-e07e-4cce-bc10-d6681f1c6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('token-classification', 'ckpt', aggregation_strategy='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57324db-29b2-44d6-b831-afb24bdb0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Heat the olive oil in a frying pan, add the onion and cook for 5 minutes until softened and starting to turn golden. Set aside.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ec687-a050-4e0c-8442-7344a4a26f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'ingredient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb84f-9b0a-4e4c-aa09-ffd59c9d76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314778b2-1774-4ecd-ab24-8b19ba841cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Introducing the best 4 Korean BBQ restaurants in Jamsil, a hot place where Lotte Tower, Seokchon Lake and Sonridan-gil are located in.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343be41c-b0e0-4090-8dff-52b443c17b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa9b13-16dd-4f92-8912-2a5217ef5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a11980-89f3-4858-a384-a9e69ec0dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Anthony Edwards was the top scorer for the third game with a personal-best 21 points, and Team USA improved to 4-0 in exhibition play with a 108-86 win over Team Greece.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979215a0-b2c6-437e-b150-791f16abce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c4639-96f0-4c63-a672-7b5b0b412e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd38e97-9a09-407e-9be4-7c1fd593e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37d38-bb85-49d9-8df2-e84317dd5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The depth and frequency of craters across the frontline city of Orikhiv are a blunt example of why Ukraine needs F-16 fighter jets urgently.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66830f36-1e4f-4ee5-aa4e-a22a9902b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'weapon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24225223-0d32-4369-b68a-0accf4c9976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(nlp, text, 'country')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72076b-629c-4438-8909-93ef083b38a0",
   "metadata": {},
   "source": [
    "## 6. Push to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd5f58-c65d-4718-bf46-4e0ed200ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ckpt')\n",
    "model = AutoModelForTokenClassification.from_pretrained('ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e738d1-fae6-4a80-b1f4-d944b8f0ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('yongsun-yoon/deberta-v3-base-open-ner')\n",
    "model.push_to_hub('yongsun-yoon/deberta-v3-base-open-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dba5f9-604c-41ee-a194-4d7935010340",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('token-classification', 'yongsun-yoon/deberta-v3-base-open-ner', aggregation_strategy='simple')\n",
    "text = 'Heat the olive oil in a frying pan, add the onion and cook for 5 minutes until softened and starting to turn golden. Set aside.'\n",
    "entity_type = 'ingredient'\n",
    "input_text = f'{text}{nlp.tokenizer.sep_token}{entity_type}'\n",
    "nlp(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5b79e-ad13-4c14-8db8-c77acfa780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Heat the olive oil in a frying pan, add the onion and cook for 5 minutes until softened and starting to turn golden. Set aside.'\n",
    "entity_type = 'ingredient'\n",
    "input_text = f'{text}{nlp.tokenizer.sep_token}{entity_type}'\n",
    "nlp(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cc2c5-07de-45ae-83b3-00c639a99d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
